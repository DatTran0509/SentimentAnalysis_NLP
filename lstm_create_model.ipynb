{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils.preprocessing import tokenizeWords, readData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from createWord2VecModel import createW2VModel\n",
    "# createW2VModel(models = [\"skipgram\", \"cbow\"])\n",
    "\n",
    "def createW2VModel(models = [\"skipgram\"]):\n",
    "    models = models\n",
    "    sentences = readData(\"_UIT-VSFC/Corpus.txt\")\n",
    "    tokenizedWords = tokenizeWords(sentences)\n",
    "    #sentences=tokenizedWords,\n",
    "    '''\n",
    "    vector_size=200 \n",
    "    window=10 \n",
    "    min_count=5 \n",
    "    workers=4\n",
    "    epochs = 10\n",
    "    '''\n",
    "    # Generate Word2Vec Model\n",
    "    for model_type in models:\n",
    "        if model_type == 'skipgram':\n",
    "            model = Word2Vec(tokenizedWords, sg = 1, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        elif model_type == 'cbow':\n",
    "            model = Word2Vec(tokenizedWords, sg = 0, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        model.save(f'word2vec/{model}_model.bin')\n",
    "    \n",
    "    print(\"Succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully\n"
     ]
    }
   ],
   "source": [
    "createW2VModel(models = [\"skipgram\", \"cbow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ tương tự với 'giảng viên': [('giáo viên', 0.7961173057556152), ('thầy giáo', 0.6603303551673889), ('thầy', 0.6395307183265686), ('lịch sự', 0.6110140085220337), ('cô', 0.6044704914093018), ('hết lòng', 0.6004809141159058), ('cẩn thận', 0.5924038887023926), ('đều đặn', 0.5840983986854553), ('kỹ lưỡng', 0.5753787159919739), ('hăng say', 0.5736873149871826)]\n"
     ]
    }
   ],
   "source": [
    "# Tải mô hình skipgram đã được lưu\n",
    "w2v_skipgram = Word2Vec.load(\"word2vec/skipgram_model.bin\")\n",
    "\n",
    "# Tìm những từ tương tự với từ \"giảng viên\"\n",
    "similar_words = w2v_skipgram.wv.most_similar(\"giảng viên\")\n",
    "print(\"Các từ tương tự với 'giảng viên':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước của các vector từ: (1417, 200)\n"
     ]
    }
   ],
   "source": [
    "# In ra kích thước của các vector đã huấn luyện\n",
    "print(\"Kích thước của các vector từ:\", w2v_skipgram.wv.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06999846,  0.01738616,  0.07625765, ..., -0.13496086,\n",
       "        -0.32864907,  0.04504812],\n",
       "       [-0.06319322, -0.11086356, -0.22311804, ..., -0.27447897,\n",
       "         0.04968915, -0.04957581],\n",
       "       [ 0.09250087,  0.09543615, -0.04298763, ..., -0.27389285,\n",
       "         0.15466587, -0.09806242],\n",
       "       ...,\n",
       "       [ 0.08734204, -0.0371408 , -0.05605735, ..., -0.13211888,\n",
       "         0.08088096, -0.09598753],\n",
       "       [-0.0295964 , -0.16139153,  0.07612254, ..., -0.2673467 ,\n",
       "        -0.0628435 ,  0.00175544],\n",
       "       [ 0.06970941,  0.0062299 , -0.03256002, ..., -0.06889631,\n",
       "        -0.00192125, -0.07397635]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lấy tất cả các vector từ\n",
    "all_vectors = w2v_skipgram.wv.vectors\n",
    "all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1417, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Skipgram Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Tạo một dictionary chứa từ và vector tương ứng\n",
    "import pickle as pkl\n",
    "words = dict()\n",
    "\n",
    "for word in w2v_skipgram.wv.index_to_key:  # index_to_key chứa danh sách các từ\n",
    "    words[word] = w2v_skipgram.wv.get_vector(word)\n",
    "\n",
    "# Lưu từ điển từ và vector vào tệp .pkl\n",
    "with open(\"utils/words_dict.pkl\", 'wb') as file:\n",
    "    pkl.dump(words, file)\n",
    "\n",
    "print(\"Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vectorizer\n",
    "with open(\"utils/words_dict.pkl\", \"rb\") as file:\n",
    "    words = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from utils.preprocessing import remove_punctuation\n",
    "import numpy as np\n",
    "def tokenize_vietnamese_sentence(sentence):\n",
    "    return word_tokenize(remove_punctuation(sentence.lower()))\n",
    "\n",
    "def sent2vec(message, word_dict = words):\n",
    "    tokens = tokenize_vietnamese_sentence(message)\n",
    "    vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in word_dict.keys():\n",
    "            continue\n",
    "        token_vector = word_dict[token]\n",
    "        vectors.append(token_vector)\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec(\"thầy dạy tốt.\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "dev_df   = pd.read_csv(\"Data/dev.csv\")\n",
    "test_df  = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "X_train, y_train = train_df[\"sents\"], train_df[\"sentiments\"]\n",
    "X_dev, y_dev = dev_df[\"sents\"], dev_df[\"sentiments\"]\n",
    "X_test, y_test = test_df[\"sents\"], test_df[\"sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 1583, 3166)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(dev_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y(dataframe):\n",
    "    y = dataframe[\"sentiments\"].to_numpy().astype(int)\n",
    "    \n",
    "    all_word_vector_sequences = []\n",
    "    \n",
    "    for message in dataframe[\"sents\"]:\n",
    "      message_as_vector_seq = sent2vec(message)\n",
    "      if message_as_vector_seq.shape[0] == 0:\n",
    "        message_as_vector_seq = np.zeros(shape=(1, 200))\n",
    "\n",
    "      all_word_vector_sequences.append(message_as_vector_seq)\n",
    "    \n",
    "    return all_word_vector_sequences, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 4\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "print(len(X_train), len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 7\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.783e+03, 2.025e+03, 4.380e+02, 1.250e+02, 3.400e+01, 1.500e+01,\n",
       "        3.000e+00, 2.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([  1. ,  12.3,  23.6,  34.9,  46.2,  57.5,  68.8,  80.1,  91.4,\n",
       "        102.7, 114. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  sequence_lengths.append(len(X_train[i]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11426.000000\n",
       "mean         9.685717\n",
       "std          7.539181\n",
       "min          1.000000\n",
       "25%          5.000000\n",
       "50%          7.000000\n",
       "75%         12.000000\n",
       "max        114.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sequence_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Create sequences padding for X\n",
    "def pad_sequences(X, desired_sequence_length=205):\n",
    "  X_copy = deepcopy(X)\n",
    "\n",
    "  for i, x in enumerate(X):\n",
    "    x_seq_len = x.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - x_seq_len\n",
    "    \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    X_copy[i] = np.concatenate([x, pad])\n",
    "  \n",
    "  return np.array(X_copy).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 205, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1583, 205, 200), (1583,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = df_to_X_y(dev_df)\n",
    "X_val = pad_sequences(X_val)\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3166, 205, 200), (3166,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = df_to_X_y(test_df)\n",
    "X_test = pad_sequences(X_test)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate LSTM Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(layers.Input(shape=(205, 200)))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 205, 128)          168448    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 205, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 205, 64)           49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 205, 64)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 205, 32)           12416     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 205, 32)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6560)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 19683     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 249955 (976.39 KB)\n",
      "Trainable params: 249955 (976.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC\n",
    "\n",
    "# Import the legacy optimizer\n",
    "from keras.optimizers import legacy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Lưu trọng số tốt nhất vào một tệp cụ thể\n",
    "cp = ModelCheckpoint(filepath='models/best_weights.h5',  # Đường dẫn tệp cụ thể\n",
    "                     monitor='val_loss',                # Theo dõi 'val_loss'\n",
    "                     save_best_only=True,               # Chỉ lưu khi đạt kết quả tốt nhất\n",
    "                     save_weights_only=True,            # Chỉ lưu trọng số\n",
    "                     verbose=1)\n",
    "\n",
    "\n",
    "# Use the legacy Adam optimizer\n",
    "optimizer = legacy.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "2    5643\n",
       "0    5325\n",
       "1     458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = pd.value_counts(train_df['sentiments'])\n",
    "\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.1457276995305166, 1: 24.94759825327511, 2: 2.024809498493709}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1], 2: frequencies.sum() / frequencies[2]}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 2.0833 - accuracy: 0.7124\n",
      "Epoch 1: val_loss improved from inf to 0.53910, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 110s 298ms/step - loss: 2.0833 - accuracy: 0.7124 - val_loss: 0.5391 - val_accuracy: 0.7524\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.7555 - accuracy: 0.7683\n",
      "Epoch 2: val_loss improved from 0.53910 to 0.48861, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 104s 291ms/step - loss: 1.7555 - accuracy: 0.7683 - val_loss: 0.4886 - val_accuracy: 0.7915\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.6039 - accuracy: 0.7907\n",
      "Epoch 3: val_loss improved from 0.48861 to 0.44876, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 100s 280ms/step - loss: 1.6039 - accuracy: 0.7907 - val_loss: 0.4488 - val_accuracy: 0.8080\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.5034 - accuracy: 0.8061\n",
      "Epoch 4: val_loss improved from 0.44876 to 0.43547, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 102s 284ms/step - loss: 1.5034 - accuracy: 0.8061 - val_loss: 0.4355 - val_accuracy: 0.8219\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.4290 - accuracy: 0.8145\n",
      "Epoch 5: val_loss improved from 0.43547 to 0.34396, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 126s 353ms/step - loss: 1.4290 - accuracy: 0.8145 - val_loss: 0.3440 - val_accuracy: 0.8711\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.3428 - accuracy: 0.8228\n",
      "Epoch 6: val_loss did not improve from 0.34396\n",
      "358/358 [==============================] - 100s 279ms/step - loss: 1.3428 - accuracy: 0.8228 - val_loss: 0.4803 - val_accuracy: 0.8073\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2597 - accuracy: 0.8339\n",
      "Epoch 7: val_loss did not improve from 0.34396\n",
      "358/358 [==============================] - 117s 328ms/step - loss: 1.2597 - accuracy: 0.8339 - val_loss: 0.4722 - val_accuracy: 0.8244\n",
      "Epoch 8/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2169 - accuracy: 0.8394\n",
      "Epoch 8: val_loss did not improve from 0.34396\n",
      "358/358 [==============================] - 101s 283ms/step - loss: 1.2169 - accuracy: 0.8394 - val_loss: 0.3691 - val_accuracy: 0.8471\n",
      "Epoch 9/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1702 - accuracy: 0.8403\n",
      "Epoch 9: val_loss did not improve from 0.34396\n",
      "358/358 [==============================] - 102s 285ms/step - loss: 1.1702 - accuracy: 0.8403 - val_loss: 0.3887 - val_accuracy: 0.8389\n",
      "Epoch 10/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.0375 - accuracy: 0.8587\n",
      "Epoch 10: val_loss did not improve from 0.34396\n",
      "358/358 [==============================] - 101s 282ms/step - loss: 1.0375 - accuracy: 0.8587 - val_loss: 0.3757 - val_accuracy: 0.8566\n",
      "Epoch 11/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1755 - accuracy: 0.8394\n",
      "Epoch 11: val_loss improved from 0.34396 to 0.30247, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 101s 283ms/step - loss: 1.1755 - accuracy: 0.8394 - val_loss: 0.3025 - val_accuracy: 0.8825\n",
      "Epoch 12/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.8665\n",
      "Epoch 12: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 100s 279ms/step - loss: 0.9522 - accuracy: 0.8665 - val_loss: 0.3757 - val_accuracy: 0.8427\n",
      "Epoch 13/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.8522 - accuracy: 0.8790\n",
      "Epoch 13: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 102s 284ms/step - loss: 0.8522 - accuracy: 0.8790 - val_loss: 0.3222 - val_accuracy: 0.8781\n",
      "Epoch 14/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.8434 - accuracy: 0.8837\n",
      "Epoch 14: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 103s 289ms/step - loss: 0.8434 - accuracy: 0.8837 - val_loss: 0.3258 - val_accuracy: 0.8756\n",
      "Epoch 15/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.7710 - accuracy: 0.8948\n",
      "Epoch 15: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 100s 280ms/step - loss: 0.7710 - accuracy: 0.8948 - val_loss: 0.3970 - val_accuracy: 0.8604\n",
      "Epoch 16/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.7264 - accuracy: 0.8997\n",
      "Epoch 16: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 104s 290ms/step - loss: 0.7264 - accuracy: 0.8997 - val_loss: 0.3256 - val_accuracy: 0.8850\n",
      "Epoch 17/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.9085\n",
      "Epoch 17: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 104s 289ms/step - loss: 0.6507 - accuracy: 0.9085 - val_loss: 0.3497 - val_accuracy: 0.8743\n",
      "Epoch 18/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.9086\n",
      "Epoch 18: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 105s 292ms/step - loss: 0.6436 - accuracy: 0.9086 - val_loss: 0.3379 - val_accuracy: 0.8756\n",
      "Epoch 19/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.9130\n",
      "Epoch 19: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 105s 292ms/step - loss: 0.6255 - accuracy: 0.9130 - val_loss: 0.3516 - val_accuracy: 0.8737\n",
      "Epoch 20/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.9178\n",
      "Epoch 20: val_loss did not improve from 0.30247\n",
      "358/358 [==============================] - 102s 286ms/step - loss: 0.5696 - accuracy: 0.9178 - val_loss: 0.3580 - val_accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình với cả EarlyStopping và ModelCheckpoint\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val, y_val), \n",
    "    epochs=20, \n",
    "    callbacks=[cp],  \n",
    "    class_weight=weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Tải trọng số tốt nhất vào mô hình\n",
    "model.load_weights('models/best_weights.h5')\n",
    "\n",
    "# Lưu toàn bộ mô hình với trọng số tốt nhất\n",
    "model.save('models/best_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(layers.Input(shape=(205, 200)))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and saved your model as follows\n",
    "# model.save(\"models/lstm_model.h5\")\n",
    "\n",
    "# Now, to load the model for later use\n",
    "loaded_model = load_model(\"models/lstm_model.h5\")\n",
    "\n",
    "# You can now use the loaded_model for predictions or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 9s 83ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.0554654e-03, 9.8747092e-01, 6.4736707e-03],\n",
       "       [1.8927797e-04, 1.8338791e-07, 9.9981052e-01],\n",
       "       [8.6641749e-03, 2.7062602e-03, 9.8862958e-01],\n",
       "       ...,\n",
       "       [4.8936021e-02, 8.2032086e-04, 9.5024359e-01],\n",
       "       [5.9225112e-01, 1.7584257e-01, 2.3190632e-01],\n",
       "       [9.3624455e-01, 2.1219210e-05, 6.3734204e-02]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = (loaded_model.predict(X_test))\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded predictions to class labels\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91      1409\n",
      "           1       0.36      0.54      0.43       167\n",
      "           2       0.92      0.89      0.91      1590\n",
      "\n",
      "    accuracy                           0.87      3166\n",
      "   macro avg       0.73      0.77      0.75      3166\n",
      "weighted avg       0.89      0.87      0.88      3166\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1267   64   78]\n",
      " [  40   90   37]\n",
      " [  81   99 1410]]\n",
      "Precision:  0.8893476890832572\n",
      "Recall:  0.873973468098547\n",
      "F1-Score:  0.88045049640696\n",
      "Accuracy:  0.873973468098547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print(\"Report: \\n\",classification_report(y_test,predicted_labels))\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,predicted_labels))\n",
    "print(\"Precision: \",precision_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Recall: \",recall_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"F1-Score: \",f1_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Accuracy: \",accuracy_score(y_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_sequence_length = 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "lstm_model = load_model(\"models/lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 205, 200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thầy giảng bài hay, cho ít bài tập\"\n",
    "def pad_sequence_sentence(sentence):\n",
    "    array = sent2vec(sentence)\n",
    "    arr_seq_len = array.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - arr_seq_len\n",
    "        \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    array = np.array(np.concatenate([array, pad]))\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array.astype(float)\n",
    "array = pad_sequence_sentence(sentence)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 685ms/step\n",
      "Sentiment:  Tích Cực\n"
     ]
    }
   ],
   "source": [
    "from utils.classify import feedbackSentimentAnalysis\n",
    "\n",
    "prediction = np.argmax(lstm_model.predict(array) > 0.5)\n",
    "sentiment = feedbackSentimentAnalysis(prediction)\n",
    "print(\"Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
