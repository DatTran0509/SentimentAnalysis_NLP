{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils.preprocessing import tokenizeWords, readData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from createWord2VecModel import createW2VModel\n",
    "# createW2VModel(models = [\"skipgram\", \"cbow\"])\n",
    "\n",
    "def createW2VModel(models = [\"skipgram\"]):\n",
    "    models = models\n",
    "    sentences = readData(\"_UIT-VSFC/Corpus.txt\")\n",
    "    tokenizedWords = tokenizeWords(sentences)\n",
    "    #sentences=tokenizedWords,\n",
    "    '''\n",
    "    vector_size=200 \n",
    "    window=10 \n",
    "    min_count=5 \n",
    "    workers=4\n",
    "    epochs = 10\n",
    "    '''\n",
    "    # Generate Word2Vec Model\n",
    "    for model_type in models:\n",
    "        if model_type == 'skipgram':\n",
    "            model = Word2Vec(tokenizedWords, sg = 1, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        elif model_type == 'cbow':\n",
    "            model = Word2Vec(tokenizedWords, sg = 0, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        model.save(f'word2vec/{model}_model.bin')\n",
    "    \n",
    "    print(\"Succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully\n"
     ]
    }
   ],
   "source": [
    "createW2VModel(models = [\"skipgram\", \"cbow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ tương tự với 'giảng viên': [('giáo viên', 0.7961173057556152), ('thầy giáo', 0.6603303551673889), ('thầy', 0.6395307183265686), ('lịch sự', 0.6110140085220337), ('cô', 0.6044704914093018), ('hết lòng', 0.6004809141159058), ('cẩn thận', 0.5924038887023926), ('đều đặn', 0.5840983986854553), ('kỹ lưỡng', 0.5753787159919739), ('hăng say', 0.5736873149871826)]\n"
     ]
    }
   ],
   "source": [
    "# Tải mô hình skipgram đã được lưu\n",
    "w2v_skipgram = Word2Vec.load(\"word2vec/skipgram_model.bin\")\n",
    "\n",
    "# Tìm những từ tương tự với từ \"giảng viên\"\n",
    "similar_words = w2v_skipgram.wv.most_similar(\"giảng viên\")\n",
    "print(\"Các từ tương tự với 'giảng viên':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước của các vector từ: (1417, 200)\n"
     ]
    }
   ],
   "source": [
    "# In ra kích thước của các vector đã huấn luyện\n",
    "print(\"Kích thước của các vector từ:\", w2v_skipgram.wv.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06999846,  0.01738616,  0.07625765, ..., -0.13496086,\n",
       "        -0.32864907,  0.04504812],\n",
       "       [-0.06319322, -0.11086356, -0.22311804, ..., -0.27447897,\n",
       "         0.04968915, -0.04957581],\n",
       "       [ 0.09250087,  0.09543615, -0.04298763, ..., -0.27389285,\n",
       "         0.15466587, -0.09806242],\n",
       "       ...,\n",
       "       [ 0.08734204, -0.0371408 , -0.05605735, ..., -0.13211888,\n",
       "         0.08088096, -0.09598753],\n",
       "       [-0.0295964 , -0.16139153,  0.07612254, ..., -0.2673467 ,\n",
       "        -0.0628435 ,  0.00175544],\n",
       "       [ 0.06970941,  0.0062299 , -0.03256002, ..., -0.06889631,\n",
       "        -0.00192125, -0.07397635]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lấy tất cả các vector từ\n",
    "all_vectors = w2v_skipgram.wv.vectors\n",
    "all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1417, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Skipgram Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Tạo một dictionary chứa từ và vector tương ứng\n",
    "import pickle as pkl\n",
    "words = dict()\n",
    "\n",
    "for word in w2v_skipgram.wv.index_to_key:  # index_to_key chứa danh sách các từ\n",
    "    words[word] = w2v_skipgram.wv.get_vector(word)\n",
    "\n",
    "# Lưu từ điển từ và vector vào tệp .pkl\n",
    "with open(\"utils/words_dict.pkl\", 'wb') as file:\n",
    "    pkl.dump(words, file)\n",
    "\n",
    "print(\"Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vectorizer\n",
    "with open(\"utils/words_dict.pkl\", \"rb\") as file:\n",
    "    words = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "from utils.preprocessing import remove_punctuation\n",
    "import numpy as np\n",
    "def tokenize_vietnamese_sentence(sentence):\n",
    "    return word_tokenize(remove_punctuation(sentence.lower()))\n",
    "\n",
    "def sent2vec(message, word_dict = words):\n",
    "    tokens = tokenize_vietnamese_sentence(message)\n",
    "    vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in word_dict.keys():\n",
    "            continue\n",
    "        token_vector = word_dict[token]\n",
    "        vectors.append(token_vector)\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec(\"thầy dạy tốt.\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "dev_df   = pd.read_csv(\"Data/dev.csv\")\n",
    "test_df  = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "X_train, y_train = train_df[\"sents\"], train_df[\"sentiments\"]\n",
    "X_dev, y_dev = dev_df[\"sents\"], dev_df[\"sentiments\"]\n",
    "X_test, y_test = test_df[\"sents\"], test_df[\"sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 1583, 3166)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(dev_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y(dataframe):\n",
    "    y = dataframe[\"sentiments\"].to_numpy().astype(int)\n",
    "    \n",
    "    all_word_vector_sequences = []\n",
    "    \n",
    "    for message in dataframe[\"sents\"]:\n",
    "      message_as_vector_seq = sent2vec(message)\n",
    "      if message_as_vector_seq.shape[0] == 0:\n",
    "        message_as_vector_seq = np.zeros(shape=(1, 200))\n",
    "\n",
    "      all_word_vector_sequences.append(message_as_vector_seq)\n",
    "    \n",
    "    return all_word_vector_sequences, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 3\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "print(len(X_train), len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 5\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.174e+03, 2.439e+03, 5.870e+02, 1.460e+02, 5.000e+01, 2.100e+01,\n",
       "        5.000e+00, 1.000e+00, 1.000e+00, 2.000e+00]),\n",
       " array([ 1. , 10.7, 20.4, 30.1, 39.8, 49.5, 59.2, 68.9, 78.6, 88.3, 98. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  sequence_lengths.append(len(X_train[i]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11426.000000\n",
       "mean         9.224313\n",
       "std          7.202322\n",
       "min          1.000000\n",
       "25%          5.000000\n",
       "50%          7.000000\n",
       "75%         11.000000\n",
       "max         98.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sequence_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Create sequences padding for X\n",
    "def pad_sequences(X, desired_sequence_length=205):\n",
    "  X_copy = deepcopy(X)\n",
    "\n",
    "  for i, x in enumerate(X):\n",
    "    x_seq_len = x.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - x_seq_len\n",
    "    \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    X_copy[i] = np.concatenate([x, pad])\n",
    "  \n",
    "  return np.array(X_copy).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 205, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1583, 205, 200), (1583,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = df_to_X_y(dev_df)\n",
    "X_val = pad_sequences(X_val)\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3166, 205, 200), (3166,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = df_to_X_y(test_df)\n",
    "X_test = pad_sequences(X_test)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate LSTM Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(64, activation='relu'))  # Lớp Dense trung gian\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(3, activation='softmax'))  # Lớp đầu ra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC\n",
    "\n",
    "# Import the legacy optimizer\n",
    "from keras.optimizers import legacy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Lưu trọng số tốt nhất vào một tệp cụ thể\n",
    "cp = ModelCheckpoint(filepath='models/best_weights.h5',  # Đường dẫn tệp cụ thể\n",
    "                     monitor='val_loss',                # Theo dõi 'val_loss'\n",
    "                     save_best_only=True,               # Chỉ lưu khi đạt kết quả tốt nhất\n",
    "                     save_weights_only=True,            # Chỉ lưu trọng số\n",
    "                     verbose=1)\n",
    "\n",
    "\n",
    "# Use the legacy Adam optimizer\n",
    "optimizer = legacy.Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "2    5643\n",
       "0    5325\n",
       "1     458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = pd.value_counts(train_df['sentiments'])\n",
    "\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.1457276995305166, 1: 24.94759825327511, 2: 2.024809498493709}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1], 2: frequencies.sum() / frequencies[2]}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.9766 - accuracy: 0.7516\n",
      "Epoch 1: val_loss improved from inf to 0.41470, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 211s 575ms/step - loss: 1.9766 - accuracy: 0.7516 - val_loss: 0.4147 - val_accuracy: 0.8383\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.5718 - accuracy: 0.8128\n",
      "Epoch 2: val_loss improved from 0.41470 to 0.39870, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 229s 641ms/step - loss: 1.5718 - accuracy: 0.8128 - val_loss: 0.3987 - val_accuracy: 0.8541\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.4061 - accuracy: 0.8396\n",
      "Epoch 3: val_loss improved from 0.39870 to 0.33787, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 198s 552ms/step - loss: 1.4061 - accuracy: 0.8396 - val_loss: 0.3379 - val_accuracy: 0.8724\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.3329 - accuracy: 0.8472\n",
      "Epoch 4: val_loss did not improve from 0.33787\n",
      "358/358 [==============================] - 198s 554ms/step - loss: 1.3329 - accuracy: 0.8472 - val_loss: 0.3509 - val_accuracy: 0.8768\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2543 - accuracy: 0.8510\n",
      "Epoch 5: val_loss did not improve from 0.33787\n",
      "358/358 [==============================] - 215s 602ms/step - loss: 1.2543 - accuracy: 0.8510 - val_loss: 0.4494 - val_accuracy: 0.8282\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.2001 - accuracy: 0.8517\n",
      "Epoch 6: val_loss did not improve from 0.33787\n",
      "358/358 [==============================] - 246s 687ms/step - loss: 1.2001 - accuracy: 0.8517 - val_loss: 0.3540 - val_accuracy: 0.8566\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1566 - accuracy: 0.8510\n",
      "Epoch 7: val_loss did not improve from 0.33787\n",
      "358/358 [==============================] - 256s 714ms/step - loss: 1.1566 - accuracy: 0.8510 - val_loss: 0.3798 - val_accuracy: 0.8408\n",
      "Epoch 8/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.0671 - accuracy: 0.8645\n",
      "Epoch 8: val_loss improved from 0.33787 to 0.30713, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 251s 700ms/step - loss: 1.0671 - accuracy: 0.8645 - val_loss: 0.3071 - val_accuracy: 0.8806\n",
      "Epoch 9/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.9945 - accuracy: 0.8723\n",
      "Epoch 9: val_loss did not improve from 0.30713\n",
      "358/358 [==============================] - 247s 690ms/step - loss: 0.9945 - accuracy: 0.8723 - val_loss: 0.3444 - val_accuracy: 0.8699\n",
      "Epoch 10/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.9455 - accuracy: 0.8790\n",
      "Epoch 10: val_loss did not improve from 0.30713\n",
      "358/358 [==============================] - 247s 691ms/step - loss: 0.9455 - accuracy: 0.8790 - val_loss: 0.3486 - val_accuracy: 0.8629\n",
      "Epoch 11/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.9478 - accuracy: 0.8750\n",
      "Epoch 11: val_loss improved from 0.30713 to 0.26573, saving model to models/best_weights.h5\n",
      "358/358 [==============================] - 255s 714ms/step - loss: 0.9478 - accuracy: 0.8750 - val_loss: 0.2657 - val_accuracy: 0.8983\n",
      "Epoch 12/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.8903\n",
      "Epoch 12: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 192s 536ms/step - loss: 0.8460 - accuracy: 0.8903 - val_loss: 0.3086 - val_accuracy: 0.8869\n",
      "Epoch 13/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.8401 - accuracy: 0.8896\n",
      "Epoch 13: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 201s 561ms/step - loss: 0.8401 - accuracy: 0.8896 - val_loss: 0.2920 - val_accuracy: 0.8838\n",
      "Epoch 14/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.9052\n",
      "Epoch 14: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 181s 505ms/step - loss: 0.7369 - accuracy: 0.9052 - val_loss: 0.2918 - val_accuracy: 0.9052\n",
      "Epoch 15/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.8963\n",
      "Epoch 15: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 182s 509ms/step - loss: 0.8184 - accuracy: 0.8963 - val_loss: 0.2867 - val_accuracy: 0.8958\n",
      "Epoch 16/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6651 - accuracy: 0.9124\n",
      "Epoch 16: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 180s 502ms/step - loss: 0.6651 - accuracy: 0.9124 - val_loss: 0.2716 - val_accuracy: 0.9065\n",
      "Epoch 17/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.9150\n",
      "Epoch 17: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 186s 520ms/step - loss: 0.6559 - accuracy: 0.9150 - val_loss: 0.2749 - val_accuracy: 0.8926\n",
      "Epoch 18/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.9176\n",
      "Epoch 18: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 199s 557ms/step - loss: 0.6082 - accuracy: 0.9176 - val_loss: 0.2957 - val_accuracy: 0.9008\n",
      "Epoch 19/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.9214\n",
      "Epoch 19: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 210s 586ms/step - loss: 0.5991 - accuracy: 0.9214 - val_loss: 0.2747 - val_accuracy: 0.8989\n",
      "Epoch 20/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.9217\n",
      "Epoch 20: val_loss did not improve from 0.26573\n",
      "358/358 [==============================] - 216s 603ms/step - loss: 0.5809 - accuracy: 0.9217 - val_loss: 0.2995 - val_accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình với cả EarlyStopping và ModelCheckpoint\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val, y_val), \n",
    "    epochs=20, \n",
    "    callbacks=[cp],  \n",
    "    class_weight=weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Tải trọng số tốt nhất vào mô hình\n",
    "model.load_weights('models/best_weights.h5')\n",
    "\n",
    "# Lưu toàn bộ mô hình với trọng số tốt nhất\n",
    "model.save('models/best_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(layers.Input(shape=(205, 200)))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and saved your model as follows\n",
    "# model.save(\"models/lstm_model.h5\")\n",
    "\n",
    "# Now, to load the model for later use\n",
    "loaded_model = load_model(\"models/lstm_model.h5\")\n",
    "\n",
    "# You can now use the loaded_model for predictions or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 9s 87ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.00344941e-03, 9.75583792e-01, 2.14128140e-02],\n",
       "       [2.00956259e-02, 7.48397335e-02, 9.05064642e-01],\n",
       "       [8.66417494e-03, 2.70626019e-03, 9.88629580e-01],\n",
       "       ...,\n",
       "       [1.40151335e-02, 1.12354290e-04, 9.85872507e-01],\n",
       "       [7.10137248e-01, 5.27234655e-03, 2.84590393e-01],\n",
       "       [9.62156773e-01, 1.21266465e-03, 3.66305225e-02]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = (loaded_model.predict(X_test))\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded predictions to class labels\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      1409\n",
      "           1       0.36      0.46      0.40       167\n",
      "           2       0.90      0.92      0.91      1590\n",
      "\n",
      "    accuracy                           0.88      3166\n",
      "   macro avg       0.73      0.75      0.74      3166\n",
      "weighted avg       0.88      0.88      0.88      3166\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1231   68  110]\n",
      " [  36   76   55]\n",
      " [  58   68 1464]]\n",
      "Precision:  0.8837204534075733\n",
      "Recall:  0.8752368919772584\n",
      "F1-Score:  0.8787304405515962\n",
      "Accuracy:  0.8752368919772584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print(\"Report: \\n\",classification_report(y_test,predicted_labels))\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,predicted_labels))\n",
    "print(\"Precision: \",precision_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Recall: \",recall_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"F1-Score: \",f1_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Accuracy: \",accuracy_score(y_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_sequence_length = 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "lstm_model = load_model(\"models/lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 205, 200)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thầy giảng bài hay, cho ít bài tập\"\n",
    "def pad_sequence_sentence(sentence):\n",
    "    array = sent2vec(sentence)\n",
    "    arr_seq_len = array.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - arr_seq_len\n",
    "        \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    array = np.array(np.concatenate([array, pad]))\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array.astype(float)\n",
    "array = pad_sequence_sentence(sentence)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Sentiment:  Tích Cực\n"
     ]
    }
   ],
   "source": [
    "from utils.classify import feedbackSentimentAnalysis\n",
    "\n",
    "prediction = np.argmax(lstm_model.predict(array) > 0.5)\n",
    "sentiment = feedbackSentimentAnalysis(prediction)\n",
    "print(\"Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
