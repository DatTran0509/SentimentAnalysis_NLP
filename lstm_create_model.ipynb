{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils.preprocessing import tokenizeWords, readData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from createWord2VecModel import createW2VModel\n",
    "# createW2VModel(models = [\"skipgram\", \"cbow\"])\n",
    "\n",
    "def createW2VModel(models = [\"skipgram\"]):\n",
    "    models = models\n",
    "    sentences = readData(\"_UIT-VSFC/Corpus.txt\")\n",
    "    tokenizedWords = tokenizeWords(sentences)\n",
    "    #sentences=tokenizedWords,\n",
    "    '''\n",
    "    vector_size=200 \n",
    "    window=10 \n",
    "    min_count=5 \n",
    "    workers=4\n",
    "    epochs = 10\n",
    "    '''\n",
    "    # Generate Word2Vec Model\n",
    "    for model_type in models:\n",
    "        if model_type == 'skipgram':\n",
    "            model = Word2Vec(tokenizedWords, sg = 1, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        elif model_type == 'cbow':\n",
    "            model = Word2Vec(tokenizedWords, sg = 0, vector_size=200, window=10, min_count=5, workers=4, epochs = 10)\n",
    "        model.save(f'word2vec/{model}_model.bin')\n",
    "    \n",
    "    print(\"Succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully\n"
     ]
    }
   ],
   "source": [
    "createW2VModel(models = [\"skipgram\", \"cbow\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ tương tự với 'giảng viên': [('giáo viên', 0.7961173057556152), ('thầy giáo', 0.6603303551673889), ('thầy', 0.6395307183265686), ('lịch sự', 0.6110140085220337), ('cô', 0.6044704914093018), ('hết lòng', 0.6004809141159058), ('cẩn thận', 0.5924038887023926), ('đều đặn', 0.5840983986854553), ('kỹ lưỡng', 0.5753787159919739), ('hăng say', 0.5736873149871826)]\n"
     ]
    }
   ],
   "source": [
    "# Tải mô hình skipgram đã được lưu\n",
    "w2v_skipgram = Word2Vec.load(\"word2vec/skipgram_model.bin\")\n",
    "\n",
    "# Tìm những từ tương tự với từ \"giảng viên\"\n",
    "similar_words = w2v_skipgram.wv.most_similar(\"giảng viên\")\n",
    "print(\"Các từ tương tự với 'giảng viên':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước của các vector từ: (1417, 200)\n"
     ]
    }
   ],
   "source": [
    "# In ra kích thước của các vector đã huấn luyện\n",
    "print(\"Kích thước của các vector từ:\", w2v_skipgram.wv.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06999846,  0.01738616,  0.07625765, ..., -0.13496086,\n",
       "        -0.32864907,  0.04504812],\n",
       "       [-0.06319322, -0.11086356, -0.22311804, ..., -0.27447897,\n",
       "         0.04968915, -0.04957581],\n",
       "       [ 0.09250087,  0.09543615, -0.04298763, ..., -0.27389285,\n",
       "         0.15466587, -0.09806242],\n",
       "       ...,\n",
       "       [ 0.08734204, -0.0371408 , -0.05605735, ..., -0.13211888,\n",
       "         0.08088096, -0.09598753],\n",
       "       [-0.0295964 , -0.16139153,  0.07612254, ..., -0.2673467 ,\n",
       "        -0.0628435 ,  0.00175544],\n",
       "       [ 0.06970941,  0.0062299 , -0.03256002, ..., -0.06889631,\n",
       "        -0.00192125, -0.07397635]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lấy tất cả các vector từ\n",
    "all_vectors = w2v_skipgram.wv.vectors\n",
    "all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1417, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Skipgram Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Tạo một dictionary chứa từ và vector tương ứng\n",
    "import pickle as pkl\n",
    "words = dict()\n",
    "\n",
    "for word in w2v_skipgram.wv.index_to_key:  # index_to_key chứa danh sách các từ\n",
    "    words[word] = w2v_skipgram.wv.get_vector(word)\n",
    "\n",
    "# Lưu từ điển từ và vector vào tệp .pkl\n",
    "with open(\"utils/words_dict.pkl\", 'wb') as file:\n",
    "    pkl.dump(words, file)\n",
    "\n",
    "print(\"Đã lưu thành công từ điển từ và vector vào 'utils/words_dict.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vectorizer\n",
    "with open(\"utils/words_dict.pkl\", \"rb\") as file:\n",
    "    words = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from utils.preprocessing import remove_punctuation\n",
    "import numpy as np\n",
    "def tokenize_vietnamese_sentence(sentence):\n",
    "    return word_tokenize(remove_punctuation(sentence.lower()))\n",
    "\n",
    "def sent2vec(message, word_dict = words):\n",
    "    tokens = tokenize_vietnamese_sentence(message)\n",
    "    vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in word_dict.keys():\n",
    "            continue\n",
    "        token_vector = word_dict[token]\n",
    "        vectors.append(token_vector)\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec(\"thầy dạy tốt.\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "dev_df   = pd.read_csv(\"Data/dev.csv\")\n",
    "test_df  = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "X_train, y_train = train_df[\"sents\"], train_df[\"sentiments\"]\n",
    "X_dev, y_dev = dev_df[\"sents\"], dev_df[\"sentiments\"]\n",
    "X_test, y_test = test_df[\"sents\"], test_df[\"sentiments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 1583, 3166)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(dev_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y(dataframe):\n",
    "    y = dataframe[\"sentiments\"].to_numpy().astype(int)\n",
    "    \n",
    "    all_word_vector_sequences = []\n",
    "    \n",
    "    for message in dataframe[\"sents\"]:\n",
    "      message_as_vector_seq = sent2vec(message)\n",
    "      if message_as_vector_seq.shape[0] == 0:\n",
    "        message_as_vector_seq = np.zeros(shape=(1, 200))\n",
    "\n",
    "      all_word_vector_sequences.append(message_as_vector_seq)\n",
    "    \n",
    "    return all_word_vector_sequences, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 3\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_to_X_y(train_df)\n",
    "\n",
    "print(len(X_train), len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11426 5\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.174e+03, 2.439e+03, 5.870e+02, 1.460e+02, 5.000e+01, 2.100e+01,\n",
       "        5.000e+00, 1.000e+00, 1.000e+00, 2.000e+00]),\n",
       " array([ 1. , 10.7, 20.4, 30.1, 39.8, 49.5, 59.2, 68.9, 78.6, 88.3, 98. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  sequence_lengths.append(len(X_train[i]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11426.000000\n",
       "mean         9.224313\n",
       "std          7.202322\n",
       "min          1.000000\n",
       "25%          5.000000\n",
       "50%          7.000000\n",
       "75%         11.000000\n",
       "max         98.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sequence_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Create sequences padding for X\n",
    "def pad_sequences(X, desired_sequence_length=205):\n",
    "  X_copy = deepcopy(X)\n",
    "\n",
    "  for i, x in enumerate(X):\n",
    "    x_seq_len = x.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - x_seq_len\n",
    "    \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    X_copy[i] = np.concatenate([x, pad])\n",
    "  \n",
    "  return np.array(X_copy).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426, 205, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11426,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1583, 205, 200), (1583,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = df_to_X_y(dev_df)\n",
    "X_val = pad_sequences(X_val)\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3166, 205, 200), (3166,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = df_to_X_y(test_df)\n",
    "X_test = pad_sequences(X_test)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate LSTM Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(layers.Input(shape=(205, 200)))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(64, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 205, 128)          168448    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 205, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 205, 64)           49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 205, 64)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 205, 32)           12416     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 205, 32)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6560)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 19683     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 249955 (976.39 KB)\n",
      "Trainable params: 249955 (976.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Import the legacy optimizer\n",
    "from keras.optimizers import legacy\n",
    "\n",
    "cp = ModelCheckpoint('model/', save_best_only=True)\n",
    "\n",
    "# Use the legacy Adam optimizer\n",
    "optimizer = legacy.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiments\n",
       "2    5643\n",
       "0    5325\n",
       "1     458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = pd.value_counts(train_df['sentiments'])\n",
    "\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.1457276995305166, 1: 24.94759825327511, 2: 2.024809498493709}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1], 2: frequencies.sum() / frequencies[2]}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.9730 - accuracy: 0.7513INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 113s 307ms/step - loss: 1.9730 - accuracy: 0.7513 - val_loss: 0.4732 - val_accuracy: 0.7997\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 101s 282ms/step - loss: 1.5476 - accuracy: 0.8065 - val_loss: 0.5508 - val_accuracy: 0.7890\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.3940 - accuracy: 0.8196INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 106s 296ms/step - loss: 1.3940 - accuracy: 0.8196 - val_loss: 0.3329 - val_accuracy: 0.8680\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 103s 289ms/step - loss: 1.2823 - accuracy: 0.8356 - val_loss: 0.3537 - val_accuracy: 0.8636\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 109s 305ms/step - loss: 1.2165 - accuracy: 0.8454 - val_loss: 0.5339 - val_accuracy: 0.7808\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 1.1744 - accuracy: 0.8475INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 111s 309ms/step - loss: 1.1744 - accuracy: 0.8475 - val_loss: 0.2701 - val_accuracy: 0.8970\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - 106s 295ms/step - loss: 1.0393 - accuracy: 0.8585 - val_loss: 0.3296 - val_accuracy: 0.8705\n",
      "Epoch 8/20\n",
      "358/358 [==============================] - 105s 292ms/step - loss: 0.9987 - accuracy: 0.8661 - val_loss: 0.4451 - val_accuracy: 0.8313\n",
      "Epoch 9/20\n",
      "358/358 [==============================] - 105s 293ms/step - loss: 0.9027 - accuracy: 0.8773 - val_loss: 0.2899 - val_accuracy: 0.8718\n",
      "Epoch 10/20\n",
      "358/358 [==============================] - 111s 310ms/step - loss: 0.8134 - accuracy: 0.8871 - val_loss: 0.4157 - val_accuracy: 0.8478\n",
      "Epoch 11/20\n",
      "358/358 [==============================] - 100s 280ms/step - loss: 0.8215 - accuracy: 0.8931 - val_loss: 0.3276 - val_accuracy: 0.8743\n",
      "Epoch 12/20\n",
      "358/358 [==============================] - 105s 294ms/step - loss: 0.7148 - accuracy: 0.9056 - val_loss: 0.3122 - val_accuracy: 0.8926\n",
      "Epoch 13/20\n",
      "358/358 [==============================] - 101s 283ms/step - loss: 0.6620 - accuracy: 0.9096 - val_loss: 0.4174 - val_accuracy: 0.8515\n",
      "Epoch 14/20\n",
      "358/358 [==============================] - 111s 310ms/step - loss: 0.7012 - accuracy: 0.9060 - val_loss: 0.3174 - val_accuracy: 0.8907\n",
      "Epoch 15/20\n",
      "358/358 [==============================] - 119s 333ms/step - loss: 0.6196 - accuracy: 0.9155 - val_loss: 0.2941 - val_accuracy: 0.9052\n",
      "Epoch 16/20\n",
      "358/358 [==============================] - 121s 338ms/step - loss: 0.5373 - accuracy: 0.9268 - val_loss: 0.3242 - val_accuracy: 0.8977\n",
      "Epoch 17/20\n",
      "358/358 [==============================] - 118s 328ms/step - loss: 0.5284 - accuracy: 0.9293 - val_loss: 0.3208 - val_accuracy: 0.8926\n",
      "Epoch 18/20\n",
      "358/358 [==============================] - 108s 302ms/step - loss: 0.4981 - accuracy: 0.9328 - val_loss: 0.3050 - val_accuracy: 0.9160\n",
      "Epoch 19/20\n",
      "358/358 [==============================] - 105s 294ms/step - loss: 0.5261 - accuracy: 0.9309 - val_loss: 0.3389 - val_accuracy: 0.8869\n",
      "Epoch 20/20\n",
      "358/358 [==============================] - 109s 304ms/step - loss: 0.4757 - accuracy: 0.9344 - val_loss: 0.3555 - val_accuracy: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x33d3c8210>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, callbacks=[cp], class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and saved your model as follows\n",
    "model.save(\"models/lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Assuming you have trained and saved your model as follows\n",
    "# model.save(\"models/lstm_model.h5\")\n",
    "\n",
    "# Now, to load the model for later use\n",
    "loaded_model = load_model(\"models/lstm_model.keras\")\n",
    "\n",
    "# You can now use the loaded_model for predictions or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 9s 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.0639720e-02, 9.3267626e-01, 6.6839773e-03],\n",
       "       [1.3310155e-04, 1.5717640e-07, 9.9986672e-01],\n",
       "       [1.0478409e-03, 3.4112400e-05, 9.9891806e-01],\n",
       "       ...,\n",
       "       [1.0204375e-03, 1.2682542e-06, 9.9897838e-01],\n",
       "       [4.9579662e-01, 1.3284430e-01, 3.7135908e-01],\n",
       "       [9.8958659e-01, 1.5357566e-04, 1.0259798e-02]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = (loaded_model.predict(X_test))\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded predictions to class labels\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1409\n",
      "           1       0.42      0.48      0.45       167\n",
      "           2       0.93      0.92      0.92      1590\n",
      "\n",
      "    accuracy                           0.90      3166\n",
      "   macro avg       0.76      0.77      0.77      3166\n",
      "weighted avg       0.90      0.90      0.90      3166\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1301   48   60]\n",
      " [  41   80   46]\n",
      " [  70   62 1458]]\n",
      "Precision:  0.9004388745515286\n",
      "Recall:  0.8967150979153506\n",
      "F1-Score:  0.8984464339563112\n",
      "Accuracy:  0.8967150979153506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print(\"Report: \\n\",classification_report(y_test,predicted_labels))\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,predicted_labels))\n",
    "print(\"Precision: \",precision_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Recall: \",recall_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"F1-Score: \",f1_score(y_test,predicted_labels,average=\"weighted\"))\n",
    "print(\"Accuracy: \",accuracy_score(y_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_sequence_length = 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "lstm_model = load_model(\"models/lstm_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 205, 200)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thầy giảng bài hay, cho ít bài tập\"\n",
    "def pad_sequence_sentence(sentence):\n",
    "    array = sent2vec(sentence)\n",
    "    arr_seq_len = array.shape[0]\n",
    "    sequence_length_difference = desired_sequence_length - arr_seq_len\n",
    "        \n",
    "    pad = np.zeros(shape=(sequence_length_difference, 200))\n",
    "\n",
    "    array = np.array(np.concatenate([array, pad]))\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array.astype(float)\n",
    "array = pad_sequence_sentence(sentence)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "Sentiment:  Tích Cực\n"
     ]
    }
   ],
   "source": [
    "from utils.classify import feedbackSentimentAnalysis\n",
    "\n",
    "prediction = np.argmax(lstm_model.predict(array) > 0.5)\n",
    "sentiment = feedbackSentimentAnalysis(prediction)\n",
    "print(\"Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
